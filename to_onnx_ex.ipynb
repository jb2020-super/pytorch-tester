{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some standard imports\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.onnx\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Super Resolution model definition in PyTorch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "\n",
    "class SuperResolutionNet(nn.Module):\n",
    "    def __init__(self, upscale_factor, inplace=False):\n",
    "        super(SuperResolutionNet, self).__init__()\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=inplace)\n",
    "        self.conv1 = nn.Conv2d(1, 64, (5, 5), (1, 1), (2, 2))\n",
    "        self.conv2 = nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1))\n",
    "        self.conv3 = nn.Conv2d(64, 32, (3, 3), (1, 1), (1, 1))\n",
    "        self.conv4 = nn.Conv2d(32, upscale_factor ** 2, (3, 3), (1, 1), (1, 1))\n",
    "        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.pixel_shuffle(self.conv4(x))\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        init.orthogonal_(self.conv1.weight, init.calculate_gain('relu'))\n",
    "        init.orthogonal_(self.conv2.weight, init.calculate_gain('relu'))\n",
    "        init.orthogonal_(self.conv3.weight, init.calculate_gain('relu'))\n",
    "        init.orthogonal_(self.conv4.weight)\n",
    "        init.zeros_(self.conv4.bias)  \n",
    "\n",
    "# Create the super-resolution model by using the above model definition.\n",
    "model = SuperResolutionNet(upscale_factor=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "conv1.weight \t torch.Size([64, 1, 5, 5])\n",
      "conv1.bias \t torch.Size([64])\n",
      "conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "conv2.bias \t torch.Size([64])\n",
      "conv3.weight \t torch.Size([32, 64, 3, 3])\n",
      "conv3.bias \t torch.Size([32])\n",
      "conv4.weight \t torch.Size([9, 32, 3, 3])\n",
      "conv4.bias \t torch.Size([9])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "def print_state_dict(state_dict):    \n",
    "    print(len(state_dict))\n",
    "    for layer in state_dict:\n",
    "        print(layer, '\\t', state_dict[layer].shape)\n",
    "    print(state_dict['conv4.bias'])\n",
    "print_state_dict(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "conv1.weight \t torch.Size([64, 1, 5, 5])\n",
      "conv1.bias \t torch.Size([64])\n",
      "conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "conv2.bias \t torch.Size([64])\n",
      "conv3.weight \t torch.Size([32, 64, 3, 3])\n",
      "conv3.bias \t torch.Size([32])\n",
      "conv4.weight \t torch.Size([9, 32, 3, 3])\n",
      "conv4.bias \t torch.Size([9])\n",
      "tensor([-0.0151, -0.0191, -0.0362, -0.0224,  0.0548,  0.0113,  0.0529,  0.0258,\n",
      "        -0.0180])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SuperResolutionNet(\n",
       "  (relu): ReLU()\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4): Conv2d(32, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pixel_shuffle): PixelShuffle(upscale_factor=3)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pretrained model weights\n",
    "model_url = 'https://s3.amazonaws.com/pytorch/test_data/export/superres_epoch100-44c6958e.pth'\n",
    "\n",
    "model.load_state_dict(model_zoo.load_url(model_url))\n",
    "\n",
    "print_state_dict(model.state_dict())\n",
    "# set the model to inference mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input to the model\n",
    "x = torch.randn(1, 1, 224, 224, requires_grad=True)\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(model,               # model being run\n",
    "                  x,                         # model input \n",
    "                  \"D:\\\\super_resolution.onnx\",   # where to save the model (can be a file or file-like object)                  \n",
    "                  opset_version=11,          # the ONNX version to export the model to                  \n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output']  # the model's output names\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_name = 'input'\n",
    "output_name = 'output'\n",
    "torch.onnx.export(model,               # model being run\n",
    "                  x,                         # model input \n",
    "                  \"D:\\\\super_resolution_2.onnx\",   # where to save the model (can be a file or file-like object)                  \n",
    "                  opset_version=11,          # the ONNX version to export the model to                  \n",
    "                  input_names = [input_name],   # the model's input names\n",
    "                  output_names = [output_name],  # the model's output names\n",
    "                  dynamic_axes= {\n",
    "                        input_name: {0: 'batch_size', 2 : 'in_width', 3: 'int_height'},\n",
    "                        output_name: {0: 'batch_size', 2: 'out_width', 3:'out_height'}}\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m\n",
      "\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0msize\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mscale_factor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'nearest'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0malign_corners\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mrecompute_scale_factor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "Down/up samples the input to either the given :attr:`size` or the given\n",
      ":attr:`scale_factor`\n",
      "\n",
      "The algorithm used for interpolation is determined by :attr:`mode`.\n",
      "\n",
      "Currently temporal, spatial and volumetric sampling are supported, i.e.\n",
      "expected inputs are 3-D, 4-D or 5-D in shape.\n",
      "\n",
      "The input dimensions are interpreted in the form:\n",
      "`mini-batch x channels x [optional depth] x [optional height] x width`.\n",
      "\n",
      "The modes available for resizing are: `nearest`, `linear` (3D-only),\n",
      "`bilinear`, `bicubic` (4D-only), `trilinear` (5D-only), `area`\n",
      "\n",
      "Args:\n",
      "    input (Tensor): the input tensor\n",
      "    size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]):\n",
      "        output spatial size.\n",
      "    scale_factor (float or Tuple[float]): multiplier for spatial size. If `scale_factor` is a tuple,\n",
      "        its length has to match `input.dim()`.\n",
      "    mode (str): algorithm used for upsampling:\n",
      "        ``'nearest'`` | ``'linear'`` | ``'bilinear'`` | ``'bicubic'`` |\n",
      "        ``'trilinear'`` | ``'area'``. Default: ``'nearest'``\n",
      "    align_corners (bool, optional): Geometrically, we consider the pixels of the\n",
      "        input and output as squares rather than points.\n",
      "        If set to ``True``, the input and output tensors are aligned by the\n",
      "        center points of their corner pixels, preserving the values at the corner pixels.\n",
      "        If set to ``False``, the input and output tensors are aligned by the corner\n",
      "        points of their corner pixels, and the interpolation uses edge value padding\n",
      "        for out-of-boundary values, making this operation *independent* of input size\n",
      "        when :attr:`scale_factor` is kept the same. This only has an effect when :attr:`mode`\n",
      "        is ``'linear'``, ``'bilinear'``, ``'bicubic'`` or ``'trilinear'``.\n",
      "        Default: ``False``\n",
      "    recompute_scale_factor (bool, optional): recompute the scale_factor for use in the\n",
      "        interpolation calculation. If `recompute_scale_factor` is ``True``, then\n",
      "        `scale_factor` must be passed in and `scale_factor` is used to compute the\n",
      "        output `size`. The computed output `size` will be used to infer new scales for\n",
      "        the interpolation. Note that when `scale_factor` is floating-point, it may differ\n",
      "        from the recomputed `scale_factor` due to rounding and precision issues.\n",
      "        If `recomputed_scale_factor` is ``False``, then `size` or `scale_factor` will\n",
      "        be used directly for interpolation.\n",
      "\n",
      ".. note::\n",
      "    With ``mode='bicubic'``, it's possible to cause overshoot, in other words it can produce\n",
      "    negative values or values greater than 255 for images.\n",
      "    Explicitly call ``result.clamp(min=0, max=255)`` if you want to reduce the overshoot\n",
      "    when displaying the image.\n",
      "\n",
      ".. warning::\n",
      "    With ``align_corners = True``, the linearly interpolating modes\n",
      "    (`linear`, `bilinear`, and `trilinear`) don't proportionally align the\n",
      "    output and input pixels, and thus the output values can depend on the\n",
      "    input size. This was the default behavior for these modes up to version\n",
      "    0.3.1. Since then, the default behavior is ``align_corners = False``.\n",
      "    See :class:`~torch.nn.Upsample` for concrete examples on how this\n",
      "    affects the outputs.\n",
      "\n",
      ".. warning::\n",
      "    When scale_factor is specified, if recompute_scale_factor=True,\n",
      "    scale_factor is used to compute the output_size which will then\n",
      "    be used to infer new scales for the interpolation.\n",
      "    The default behavior for recompute_scale_factor changed to False\n",
      "    in 1.6.0, and scale_factor is used in the interpolation\n",
      "    calculation.\n",
      "\n",
      "Note:\n",
      "    This operation may produce nondeterministic gradients when given tensors on a CUDA device. See :doc:`/notes/randomness` for more information.\n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\administrator\\miniconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\functional.py\n",
      "\u001b[1;31mType:\u001b[0m      function\n"
     ]
    }
   ],
   "source": [
    "F.interpolate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperResolutionNet2(nn.Module):\n",
    "    def __init__(self, upscale_factor, inplace=False):\n",
    "        super(SuperResolutionNet2, self).__init__()\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=inplace)\n",
    "        self.conv1 = nn.Conv2d(1, 64, (5, 5), (1, 1), (2, 2))\n",
    "        self.conv2 = nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1))\n",
    "        self.conv3 = nn.Conv2d(64, 32, (3, 3), (1, 1), (1, 1))\n",
    "        self.conv4 = nn.Conv2d(32, upscale_factor ** 2, (3, 3), (1, 1), (1, 1))\n",
    "        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x, scale):\n",
    "        print(scale)        \n",
    "        y = F.interpolate(x, scale_factor= 1./float(scale), mode=\"bilinear\")\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.pixel_shuffle(self.conv4(x))\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        init.orthogonal_(self.conv1.weight, init.calculate_gain('relu'))\n",
    "        init.orthogonal_(self.conv2.weight, init.calculate_gain('relu'))\n",
    "        init.orthogonal_(self.conv3.weight, init.calculate_gain('relu'))\n",
    "        init.orthogonal_(self.conv4.weight)\n",
    "        init.zeros_(self.conv4.bias)  \n",
    "\n",
    "# Create the super-resolution model by using the above model definition.\n",
    "model2 = SuperResolutionNet2(upscale_factor=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_17388/1068336433.py:16: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  y = F.interpolate(x, scale_factor= 1./float(scale), mode=\"bilinear\")\n"
     ]
    }
   ],
   "source": [
    "input_name = 'input'\n",
    "output_name = 'output'\n",
    "torch.onnx.export(model2,               \n",
    "                  (x, 2),                         \n",
    "                  \"D:\\\\super_resolution_3.onnx\",   \n",
    "                  opset_version=11,          \n",
    "                  input_names = [input_name],  \n",
    "                  output_names = [output_name],\n",
    "                  dynamic_axes= {\n",
    "                        input_name: {0: 'batch_size', 2 : 'in_width', 3: 'int_height'},\n",
    "                        output_name: {0: 'batch_size', 2: 'out_width', 3:'out_height'}}\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fb82d7e5416f6a7e93ad7b4056e8fc801ae41614f9a63a1de00dbc26c70ea266"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('d2l': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
